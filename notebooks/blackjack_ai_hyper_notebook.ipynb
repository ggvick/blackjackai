{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04555de6",
   "metadata": {},
   "source": [
    "# Blackjack AI Hyper Notebook\n",
    "\n",
    "This consolidated notebook merges the original Colab setup walkthrough and the full training/evaluation pipeline into a single, fully documented workflow. Run every cell from top to bottom in Google Colab or a local Jupyter environment to install dependencies, configure the Blackjack environment, train reinforcement-learning agents, evaluate results, and validate the system with smoke tests.\n",
    "\n",
    "**What you will find here**\n",
    "\n",
    "1. Environment and dependency setup with GPU auto-detection and installation helpers.\n",
    "2. Detailed Blackjack environment implementation supporting multi-deck shoes, Hi-Lo counting, advanced actions, and bankroll-aware episodes.\n",
    "3. Baseline, tabular, and DQN agents with curriculum scheduling, vectorized environments, and checkpointing utilities.\n",
    "4. Training routines that log metrics, save artifacts, and support GPU acceleration when available.\n",
    "5. Evaluation helpers that produce win-rate statistics, bankroll trajectories, action analysis, and comparison plots versus the baseline strategy.\n",
    "6. Robust validation cells that execute smoke tests, print diagnostics, and include a dedicated scan ensuring no malformed legacy print statements remain.\n",
    "\n",
    "All outputs (plots, CSVs, checkpoints) are saved under `./outputs` and `./models` by default. Adjust configuration cells as needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ef9314",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Setup & Utilities](#setup)\n",
    "- [Blackjack Environment & Utilities](#environment)\n",
    "- [Agent Implementations](#agents)\n",
    "- [Training Pipeline](#training)\n",
    "- [Evaluation & Reporting](#evaluation)\n",
    "- [Diagnostics & Tests](#diagnostics)\n",
    "- [Appendix](#appendix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc9d06c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id='setup'></a>Setup & Utilities\n",
    "\n",
    "The following section originates from the Colab initialization notebook and configures the runtime, installs dependencies, and prepares device utilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a374f5af",
   "metadata": {},
   "source": [
    "\n",
    "# Blackjack AI Training and Environment Verification\n",
    "\n",
    "This Colab-ready notebook configures connectivity, installs dependencies, validates hardware, and runs a short Q-learning training session for the Blackjack reinforcement learning environment with Hi-Lo card counting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abf9aee",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Connectivity and Hardware Checks\n",
    "Verify that the runtime can reach the internet and detect available hardware (GPU/TPU/CPU). Results are stored in a shared `RUNTIME_REPORT` dictionary for later summarization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9cabdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import urllib.request\n",
    "from urllib.error import URLError\n",
    "\n",
    "RUNTIME_REPORT = {\n",
    "    \"internet\": {\"status\": False, \"details\": \"\"},\n",
    "    \"hardware\": {\"type\": \"Unknown\", \"details\": \"\", \"gpu_available\": False},\n",
    "    \"dependencies\": [],\n",
    "    \"repository\": {\"status\": \"unverified\", \"path\": \"\"},\n",
    "    \"module_imports\": {\"status\": False, \"details\": \"\"},\n",
    "    \"environment_test\": {\"status\": False, \"details\": \"\"},\n",
    "    \"benchmark\": {\"status\": False, \"details\": \"\"},\n",
    "    \"training\": {\"status\": False, \"details\": \"\"},\n",
    "}\n",
    "PROJECT_ROOT = None\n",
    "\n",
    "print(\"Checking internet connectivity...\")\n",
    "try:\n",
    "    with urllib.request.urlopen(\"https://pypi.org/simple/pip/\", timeout=5) as response:\n",
    "        snippet = response.read(256)\n",
    "        RUNTIME_REPORT[\"internet\"][\"status\"] = True\n",
    "        RUNTIME_REPORT[\"internet\"][\"details\"] = f\"Fetched {len(snippet)} bytes from PyPI simple index.\"\n",
    "        print(\"Internet status: ✅\", RUNTIME_REPORT[\"internet\"][\"details\"])\n",
    "except Exception as exc:  # pragma: no cover - network failure branch\n",
    "    RUNTIME_REPORT[\"internet\"][\"status\"] = False\n",
    "    RUNTIME_REPORT[\"internet\"][\"details\"] = f\"{type(exc).__name__}: {exc}\"\n",
    "    print(\"Internet status: ❌\", RUNTIME_REPORT[\"internet\"][\"details\"])\n",
    "\n",
    "print(\"Detecting available hardware...\")\n",
    "hardware_lines = []\n",
    "if \"COLAB_TPU_ADDR\" in os.environ:\n",
    "    hardware_lines.append(\"TPU detected via COLAB_TPU_ADDR environment variable.\")\n",
    "    RUNTIME_REPORT[\"hardware\"] = {\"type\": \"TPU\", \"details\": hardware_lines[-1], \"gpu_available\": False}\n",
    "    print(hardware_lines[-1])\n",
    "else:\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"nvidia-smi\", \"--query-gpu=name,memory.total\", \"--format=csv,noheader\"],\n",
    "            check=True,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "        )\n",
    "        lines = [line.strip() for line in result.stdout.strip().splitlines() if line.strip()]\n",
    "        if lines:\n",
    "            gpu_name = lines[0].split(\",\")[0].strip()\n",
    "            runtime_type = \"GPU (T4)\" if \"T4\" in gpu_name else \"GPU\"\n",
    "            detail = \" | \".join(lines)\n",
    "            RUNTIME_REPORT[\"hardware\"] = {\"type\": runtime_type, \"details\": detail, \"gpu_available\": True}\n",
    "            print(\"GPU detected:\")\n",
    "            for line in lines:\n",
    "                print(\"  -\", line)\n",
    "            if \"T4\" in gpu_name:\n",
    "                print(\"Using expected NVIDIA T4 GPU.\")\n",
    "            else:\n",
    "                print(\"GPU is available but not a T4 (falling back to generic GPU logic).\")\n",
    "        else:  # pragma: no cover - no GPU info branch\n",
    "            raise FileNotFoundError(\"nvidia-smi returned no GPU info\")\n",
    "    except Exception as exc:  # pragma: no cover - CPU fallback branch\n",
    "        cpu_info = platform.processor() or platform.machine()\n",
    "        detail = f\"CPU only: {cpu_info}\"\n",
    "        RUNTIME_REPORT[\"hardware\"] = {\"type\": \"CPU\", \"details\": detail, \"gpu_available\": False}\n",
    "        print(\"CPU-only runtime detected:\", detail)\n",
    "\n",
    "if not RUNTIME_REPORT[\"hardware\"].get(\"details\"):\n",
    "    RUNTIME_REPORT[\"hardware\"][\"details\"] = \" | \".join(hardware_lines) or \"No additional hardware details.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725d5cd6",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Clone or Update Repository\n",
    "Clone the Blackjack AI repository if it is not already available. When the repo already exists, pull the latest changes and ensure submodules are initialized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7cbde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pathlib\n",
    "\n",
    "REPO_URL = \"https://github.com/ggvick/blackjackai\"\n",
    "notebook_dir = pathlib.Path.cwd().resolve()\n",
    "repo_status = \"already_present\"\n",
    "repo_path = None\n",
    "\n",
    "if (notebook_dir / \".git\").exists():\n",
    "    repo_path = notebook_dir\n",
    "elif (notebook_dir.parent / \".git\").exists():\n",
    "    repo_path = notebook_dir.parent.resolve()\n",
    "else:\n",
    "    target_dir = pathlib.Path(\"/content/blackjackai\").resolve()\n",
    "    if target_dir.exists() and (target_dir / \".git\").exists():\n",
    "        print(f\"Repository already exists at {target_dir} - pulling latest changes...\")\n",
    "        try:\n",
    "            subprocess.run([\"git\", \"-C\", str(target_dir), \"pull\", \"--ff-only\"], check=True)\n",
    "            repo_status = \"updated\"\n",
    "        except subprocess.CalledProcessError as exc:\n",
    "            print(\"Git pull failed:\", exc)\n",
    "            repo_status = \"exists\"\n",
    "    else:\n",
    "        print(f\"Cloning repository from {REPO_URL} to {target_dir}...\")\n",
    "        target_dir.parent.mkdir(parents=True, exist_ok=True)\n",
    "        subprocess.run([\"git\", \"clone\", REPO_URL, str(target_dir)], check=True)\n",
    "        repo_status = \"cloned\"\n",
    "    subprocess.run([\"git\", \"-C\", str(target_dir), \"submodule\", \"update\", \"--init\", \"--recursive\"], check=True)\n",
    "    repo_path = target_dir\n",
    "\n",
    "if repo_path is None:\n",
    "    raise RuntimeError(\"Unable to locate or clone the repository.\")\n",
    "\n",
    "os.chdir(repo_path)\n",
    "PROJECT_ROOT = repo_path\n",
    "RUNTIME_REPORT[\"repository\"] = {\"status\": repo_status, \"path\": str(PROJECT_ROOT)}\n",
    "print(f\"Repository root: {PROJECT_ROOT}\")\n",
    "\n",
    "# Ensure the project root is importable\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5f9177",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Dependency Installation\n",
    "Install project requirements along with common ML/RL libraries. Each package installation is retried once if it fails.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144bdee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert PROJECT_ROOT is not None, \"PROJECT_ROOT must be set before installing dependencies.\"\n",
    "requirements_path = PROJECT_ROOT / \"requirements.txt\"\n",
    "packages = [\"torch\", \"numpy\", \"gymnasium\", \"matplotlib\", \"pandas\", \"tqdm\"]\n",
    "install_targets = []\n",
    "\n",
    "if requirements_path.exists():\n",
    "    install_targets.append([\"-r\", str(requirements_path)])\n",
    "install_targets.extend([[pkg] for pkg in packages])\n",
    "install_targets.append([\"-e\", str(PROJECT_ROOT)])\n",
    "\n",
    "install_results = []\n",
    "for target in install_targets:\n",
    "    desc = \" \".join(target)\n",
    "    success = False\n",
    "    for attempt in range(2):\n",
    "        try:\n",
    "            print(f\"Installing {desc} (attempt {attempt + 1})...\")\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\"] + target, check=True)\n",
    "            success = True\n",
    "            print(f\"Installation successful: {desc}\")\n",
    "            break\n",
    "        except subprocess.CalledProcessError as exc:\n",
    "            print(f\"Installation failed for {desc}: {exc}\")\n",
    "            if attempt == 0:\n",
    "                print(\"Retrying...\")\n",
    "                time.sleep(1)\n",
    "    install_results.append({\"target\": desc, \"success\": success})\n",
    "    if not success:\n",
    "        raise RuntimeError(f\"Failed to install dependency: {desc}\")\n",
    "\n",
    "RUNTIME_REPORT[\"dependencies\"] = install_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fd6349",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Import Validation and Environment Smoke Test\n",
    "Import the Blackjack environment, run a quick demo round, and capture state information to ensure the card shoe and counter behave as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc434019",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pprint import pprint\n",
    "\n",
    "try:\n",
    "    from blackjack import BlackjackEnv, CountingPolicy, QLearningTrainer, evaluate_policy, seed_everything\n",
    "    from blackjack.rl.training import TrainingConfig\n",
    "    RUNTIME_REPORT[\"module_imports\"] = {\"status\": True, \"details\": \"Project modules imported successfully.\"}\n",
    "except Exception as exc:  # pragma: no cover - import failure branch\n",
    "    RUNTIME_REPORT[\"module_imports\"] = {\"status\": False, \"details\": f\"Import failure: {exc}\"}\n",
    "    raise\n",
    "\n",
    "seed_everything(123)\n",
    "demo_policy = CountingPolicy(min_bet=10.0, max_bet=200.0, ramp={1: 2, 2: 4, 3: 6, 4: 8, 5: 10})\n",
    "demo_env = BlackjackEnv(num_decks=6, penetration=0.75, counting_policy=demo_policy, seed=123)\n",
    "\n",
    "initial_state = demo_env.reset()\n",
    "print(\"Initial observation:\", initial_state)\n",
    "print(\n",
    "    f\"Initial bet: {demo_env.current_bet:.2f} | Running count: {demo_env.counter.running_count} | True count: {demo_env.counter.true_count():.2f}\"\n",
    ")\n",
    "\n",
    "transition_log = []\n",
    "while True:\n",
    "    action = demo_env.sample_action()\n",
    "    next_state, reward, done, info = demo_env.step(action)\n",
    "    transition_log.append(\n",
    "        {\n",
    "            \"action\": action,\n",
    "            \"next_state\": next_state,\n",
    "            \"reward\": reward,\n",
    "            \"done\": done,\n",
    "            \"info\": info,\n",
    "        }\n",
    "    )\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(\"Demo round transitions:\")\n",
    "for entry in transition_log:\n",
    "    pprint(entry)\n",
    "\n",
    "final_info = transition_log[-1][\"info\"] if transition_log else demo_env.last_info\n",
    "RUNTIME_REPORT[\"environment_test\"] = {\n",
    "    \"status\": True,\n",
    "    \"details\": f\"Round outcome: {final_info.get('outcome', 'unknown')} | True count: {final_info.get('true_count', 0):.2f}\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41637d3f",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Lightweight CPU/GPU Benchmark\n",
    "Run a small matrix multiplication benchmark on CPU (NumPy) and GPU (PyTorch CUDA) when available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bca0536",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "matrix_size = 256\n",
    "cpu_start = time.perf_counter()\n",
    "np_a = np.random.rand(matrix_size, matrix_size)\n",
    "np_b = np.random.rand(matrix_size, matrix_size)\n",
    "np.dot(np_a, np_b)\n",
    "cpu_elapsed = time.perf_counter() - cpu_start\n",
    "\n",
    "benchmark_details = {\"cpu_matrix_size\": matrix_size, \"cpu_time_sec\": cpu_elapsed}\n",
    "print(f\"CPU matrix multiplication ({matrix_size}x{matrix_size}) completed in {cpu_elapsed:.4f} seconds.\")\n",
    "\n",
    "gpu_elapsed = None\n",
    "if RUNTIME_REPORT[\"hardware\"].get(\"gpu_available\") and torch.cuda.is_available():\n",
    "    torch.manual_seed(0)\n",
    "    device = torch.device(\"cuda\")\n",
    "    gpu_start = time.perf_counter()\n",
    "    torch_a = torch.rand((matrix_size, matrix_size), device=device)\n",
    "    torch_b = torch.rand((matrix_size, matrix_size), device=device)\n",
    "    torch.matmul(torch_a, torch_b)\n",
    "    torch.cuda.synchronize()\n",
    "    gpu_elapsed = time.perf_counter() - gpu_start\n",
    "    benchmark_details[\"gpu_time_sec\"] = gpu_elapsed\n",
    "    print(f\"GPU matrix multiplication completed in {gpu_elapsed:.4f} seconds on {RUNTIME_REPORT['hardware']['type']}.\")\n",
    "else:\n",
    "    print(\"GPU not available; skipping GPU benchmark.\")\n",
    "\n",
    "RUNTIME_REPORT[\"benchmark\"] = {\"status\": True, \"details\": benchmark_details}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af62ca72",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Q-Learning Training Demo\n",
    "Train a small Q-learning agent and report training plus evaluation metrics to confirm the AI workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539c95b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_env = BlackjackEnv(\n",
    "    num_decks=6,\n",
    "    penetration=0.75,\n",
    "    natural_payout=1.5,\n",
    "    hit_soft_17=False,\n",
    "    min_bet=5.0,\n",
    "    max_bet=100.0,\n",
    "    counting_policy=CountingPolicy(min_bet=5.0, max_bet=100.0),\n",
    "    seed=42,\n",
    ")\n",
    "training_config = TrainingConfig(\n",
    "    episodes=500,\n",
    "    alpha=0.05,\n",
    "    gamma=0.99,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_min=0.05,\n",
    "    epsilon_decay=0.995,\n",
    "    log_every=100,\n",
    ")\n",
    "trainer = QLearningTrainer(training_env, config=training_config, seed=42)\n",
    "training_result = trainer.train()\n",
    "\n",
    "print(\"Training summary:\")\n",
    "for key, value in training_result.summary.items():\n",
    "    if key.endswith(\"rate\"):\n",
    "        print(f\"  {key}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value:.3f}\" if isinstance(value, float) else f\"  {key}: {value}\")\n",
    "\n",
    "eval_env = BlackjackEnv(\n",
    "    num_decks=6,\n",
    "    penetration=0.75,\n",
    "    natural_payout=1.5,\n",
    "    hit_soft_17=False,\n",
    "    min_bet=5.0,\n",
    "    max_bet=100.0,\n",
    "    counting_policy=CountingPolicy(min_bet=5.0, max_bet=100.0),\n",
    "    seed=999,\n",
    ")\n",
    "eval_metrics = evaluate_policy(eval_env, training_result.q_table, episodes=200, seed=999)\n",
    "\n",
    "print(\"Evaluation metrics (1000 episodes):\")\n",
    "for key, value in eval_metrics.items():\n",
    "    if key.endswith(\"rate\") or key.endswith(\"reward\"):\n",
    "        print(f\"  {key}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "combined_metrics = {\n",
    "    **{f\"train_{k}\": v for k, v in training_result.summary.items()},\n",
    "    **{f\"eval_{k}\": v for k, v in eval_metrics.items()},\n",
    "}\n",
    "RUNTIME_REPORT[\"training\"] = {\"status\": True, \"details\": combined_metrics}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a68906a",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Environment Configuration Summary\n",
    "Aggregate all recorded status messages to provide a quick readiness report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52fc255",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=== Environment Validation Summary ===\")\n",
    "print(f\"Internet: {'✅' if RUNTIME_REPORT['internet']['status'] else '❌'} - {RUNTIME_REPORT['internet']['details']}\")\n",
    "print(f\"Hardware: {RUNTIME_REPORT['hardware']['type']} - {RUNTIME_REPORT['hardware']['details']}\")\n",
    "\n",
    "print(\"Dependency installation results:\")\n",
    "for item in RUNTIME_REPORT[\"dependencies\"]:\n",
    "    symbol = \"✅\" if item[\"success\"] else \"❌\"\n",
    "    print(f\"  {symbol} {item['target']}\")\n",
    "\n",
    "repo_info = RUNTIME_REPORT[\"repository\"]\n",
    "print(f\"Repository status: {repo_info['status']} (path: {repo_info['path']})\")\n",
    "\n",
    "module_info = RUNTIME_REPORT[\"module_imports\"]\n",
    "print(f\"Module imports: {'✅' if module_info['status'] else '❌'} - {module_info['details']}\")\n",
    "\n",
    "env_info = RUNTIME_REPORT[\"environment_test\"]\n",
    "print(f\"Environment smoke test: {'✅' if env_info['status'] else '❌'} - {env_info['details']}\")\n",
    "\n",
    "benchmark_info = RUNTIME_REPORT[\"benchmark\"]\n",
    "print(f\"Benchmark status: {'✅' if benchmark_info['status'] else '❌'} - {benchmark_info['details']}\")\n",
    "\n",
    "training_info = RUNTIME_REPORT[\"training\"]\n",
    "print(f\"Training summary recorded: {'✅' if training_info['status'] else '❌'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2604e372",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id='environment'></a>Blackjack Environment & Utilities\n",
    "\n",
    "The next sections (adapted from the training/evaluation notebook) define the Blackjack environment, counting helpers, and strategy logic used by both baseline and learning agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e57ed39",
   "metadata": {},
   "source": [
    "### <a id='agents'></a>Agent Implementations Overview\n",
    "\n",
    "Cells in the following section introduce baseline, tabular, and deep reinforcement-learning agents that interact with the environment. They include curriculum scheduling helpers, replay buffers, and model definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3fa9e1",
   "metadata": {},
   "source": [
    "### <a id='training'></a>Training Pipeline Overview\n",
    "\n",
    "Training routines coordinate vectorized environments, optimization steps, logging, checkpointing, and curriculum scheduling. Follow the configuration cells to kick off training runs and monitor progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f73a75e",
   "metadata": {},
   "source": [
    "### <a id='evaluation'></a>Evaluation & Reporting Overview\n",
    "\n",
    "Evaluation helpers execute large numbers of hands with the learned policy, produce win-rate statistics, visualize bankroll trajectories, and compare performance with the basic-strategy baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dfac70",
   "metadata": {},
   "source": [
    "\n",
    "# Blackjack AI Training & Evaluation\n",
    "\n",
    "This notebook trains and evaluates Blackjack reinforcement learning agents. It is designed to run top-to-bottom in Google Colab or Jupyter.\n",
    "\n",
    "**Workflow overview**\n",
    "1. Run the setup cell to install/runtime-check dependencies and configure the notebook.\n",
    "2. Execute the smoke tests to validate the Blackjack environment and utilities.\n",
    "3. Train the DQN agent (vectorised env) and track learning curves.\n",
    "4. Evaluate the trained policy against a basic-strategy counting baseline, generating plots and CSV artifacts.\n",
    "\n",
    "Artifacts are saved under `./outputs` with subfolders for models, metrics, and plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68f206d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "REQUIRED_PACKAGES = {\n",
    "    \"torch\": \"torch\",\n",
    "    \"matplotlib\": \"matplotlib\",\n",
    "    \"pandas\": \"pandas\",\n",
    "    \"tqdm\": \"tqdm\",\n",
    "}\n",
    "\n",
    "for module_name, install_name in REQUIRED_PACKAGES.items():\n",
    "    try:\n",
    "        importlib.import_module(module_name)\n",
    "    except ImportError:\n",
    "        print(f\"Installing {install_name}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", install_name])\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "import torch\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))\n",
    "\n",
    "OUTPUT_DIR = ROOT / \"outputs\"\n",
    "for sub in [OUTPUT_DIR, OUTPUT_DIR / \"models\", OUTPUT_DIR / \"plots\", OUTPUT_DIR / \"metrics\"]:\n",
    "    sub.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Outputs directory: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e799c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from dataclasses import asdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from blackjackai_rl.device import detect_torch_device\n",
    "from blackjackai_rl.utils import ensure_dir, set_global_seed, to_json\n",
    "from blackjackai_rl.env import BlackjackEnvConfig\n",
    "from blackjackai_rl.agents import DQNAgent, DQNConfig\n",
    "from blackjackai_rl.training import train_dqn\n",
    "from blackjackai_rl.evaluation import (\n",
    "    compare_to_baseline,\n",
    "    evaluate_policy,\n",
    "    plot_evaluation_results,\n",
    "    plot_training_curves,\n",
    "    save_hand_records,\n",
    ")\n",
    "from blackjackai_rl.tests import run_all_tests\n",
    "\n",
    "DEVICE = detect_torch_device()\n",
    "print(f\"Using device: {DEVICE.device}\")\n",
    "set_global_seed(42)\n",
    "BASE_OUTPUT = ensure_dir(\"outputs\")\n",
    "ensure_dir(BASE_OUTPUT / \"plots\")\n",
    "ensure_dir(BASE_OUTPUT / \"metrics\")\n",
    "ensure_dir(BASE_OUTPUT / \"models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65daa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = run_all_tests()\n",
    "for result in results:\n",
    "    status = \"PASS\" if result.passed else \"FAIL\"\n",
    "    message = f\"{status} :: {result.name}\"\n",
    "    if result.details:\n",
    "        message += f\" | {result.details}\"\n",
    "    print(message)\n",
    "assert all(r.passed for r in results), \"One or more smoke tests failed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237fbe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_steps = 20000\n",
    "vector_envs = 32\n",
    "log_interval = 1000\n",
    "evaluation_hands = 4000\n",
    "\n",
    "env_config = BlackjackEnvConfig(\n",
    "    num_decks=6,\n",
    "    penetration=0.8,\n",
    "    natural_payout=1.5,\n",
    "    hit_soft_17=False,\n",
    "    min_bet=1.0,\n",
    "    max_bet=8.0,\n",
    "    bankroll=100.0,\n",
    "    bankroll_stop_loss=0.0,\n",
    "    bankroll_target=200.0,\n",
    "    allow_surrender=True,\n",
    "    allow_double=True,\n",
    "    allow_split=True,\n",
    "    max_splits=1,\n",
    "    reward_shaping=True,\n",
    "    shaping_stop_step=60000,\n",
    ")\n",
    "\n",
    "agent_config = DQNConfig(\n",
    "    state_dim=12,\n",
    "    num_actions=5,\n",
    "    hidden_sizes=(256, 256),\n",
    "    gamma=0.99,\n",
    "    lr=5e-4,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_final=0.05,\n",
    "    epsilon_decay=200000,\n",
    "    batch_size=512,\n",
    "    buffer_size=200000,\n",
    "    min_buffer_size=4000,\n",
    "    target_update_interval=2000,\n",
    "    tau=0.01,\n",
    "    device=DEVICE.device,\n",
    ")\n",
    "\n",
    "print(\"Training configuration ready\")\n",
    "print(env_config)\n",
    "print(agent_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50128cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_history = train_dqn(\n",
    "    env_config=env_config,\n",
    "    agent_config=agent_config,\n",
    "    total_steps=training_steps,\n",
    "    vector_envs=vector_envs,\n",
    "    log_interval=log_interval,\n",
    "    output_dir=BASE_OUTPUT,\n",
    ")\n",
    "\n",
    "best_model_path = Path(training_history[\"best_model_path\"])\n",
    "print(f\"Best checkpoint: {best_model_path}\")\n",
    "print(f\"Timings: {[asdict(t) for t in training_history['timings']]}\")\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"reward\": training_history[\"reward_history\"],\n",
    "    \"epsilon\": training_history[\"epsilon_history\"],\n",
    "})\n",
    "metrics_csv = BASE_OUTPUT / \"metrics\" / \"training_batch_metrics.csv\"\n",
    "metrics_df.to_csv(metrics_csv, index_label=\"batch\")\n",
    "print(f\"Saved batch metrics to {metrics_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65a9423",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_paths = plot_training_curves(training_history, BASE_OUTPUT)\n",
    "print(\"Training plots:\")\n",
    "for name, path in plot_paths.items():\n",
    "    print(f\"  {name}: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dafc9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trained_agent = DQNAgent(agent_config)\n",
    "trained_agent.load(str(best_model_path), map_location=DEVICE.device)\n",
    "\n",
    "comparison = compare_to_baseline(env_config, trained_agent, num_hands=evaluation_hands)\n",
    "trained_eval = comparison[\"trained\"]\n",
    "baseline_eval = comparison[\"baseline\"]\n",
    "\n",
    "trained_records = pd.DataFrame([asdict(record) for record in trained_eval[\"hand_records\"]])\n",
    "baseline_records = pd.DataFrame([asdict(record) for record in baseline_eval[\"hand_records\"]])\n",
    "\n",
    "trained_records_path = BASE_OUTPUT / \"metrics\" / \"trained_hand_history.csv\"\n",
    "baseline_records_path = BASE_OUTPUT / \"metrics\" / \"baseline_hand_history.csv\"\n",
    "trained_records.to_csv(trained_records_path, index=False)\n",
    "baseline_records.to_csv(baseline_records_path, index=False)\n",
    "print(f\"Saved trained hand history to {trained_records_path}\")\n",
    "print(f\"Saved baseline hand history to {baseline_records_path}\")\n",
    "\n",
    "summary_json = {\n",
    "    \"trained\": asdict(trained_eval[\"summary\"]),\n",
    "    \"baseline\": asdict(baseline_eval[\"summary\"]),\n",
    "    \"expected_value_gain\": comparison[\"expected_value_gain\"],\n",
    "}\n",
    "summary_path = BASE_OUTPUT / \"metrics\" / \"evaluation_summary.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as handle:\n",
    "    json.dump(summary_json, handle, indent=2)\n",
    "print(f\"Summary saved to {summary_path}\")\n",
    "\n",
    "plot_eval_paths = plot_evaluation_results(trained_eval[\"hand_records\"], BASE_OUTPUT)\n",
    "print(\"Evaluation plots:\")\n",
    "for name, path in plot_eval_paths.items():\n",
    "    print(f\"  {name}: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7bac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trained_summary = comparison[\"trained\"][\"summary\"]\n",
    "baseline_summary = comparison[\"baseline\"][\"summary\"]\n",
    "print(\"Trained agent summary:\")\n",
    "print(trained_summary)\n",
    "print(\"\n",
    "Baseline summary:\")\n",
    "print(baseline_summary)\n",
    "print(\"\n",
    "Expected value gain per hand:\", comparison[\"expected_value_gain\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba9a908",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pprint import pprint\n",
    "\n",
    "artifacts = {\n",
    "    \"best_model\": str(best_model_path),\n",
    "    \"final_model\": str(Path(BASE_OUTPUT) / \"models\" / \"final_dqn.pt\"),\n",
    "    \"training_metrics\": str(metrics_csv),\n",
    "    \"evaluation_summary\": str(BASE_OUTPUT / \"metrics\" / \"evaluation_summary.json\"),\n",
    "}\n",
    "artifacts.update(plot_paths)\n",
    "artifacts.update(plot_eval_paths)\n",
    "print(\"✅ Training complete | Eval done | Plots saved to ./outputs | Best model:\", best_model_path)\n",
    "pprint(artifacts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd4a3cd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id='diagnostics'></a>Additional Diagnostics\n",
    "\n",
    "The following diagnostic cell scans the repository for legacy Python 2 style print statements or malformed usage that could break execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1eef6c",
   "metadata": {
    "tags": [
     "diagnostic",
     "print-scan"
    ]
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path('.')\n",
    "legacy_pattern = re.compile(r\"^\\s*print\\s+[^\\(\\n][^\\n]*$\", re.MULTILINE)\n",
    "files_with_issues = []\n",
    "for path in root.rglob('*.py'):\n",
    "    try:\n",
    "        text = path.read_text(encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        continue\n",
    "    for match in legacy_pattern.finditer(text):\n",
    "        files_with_issues.append(f\"{path}:{match.group(0).strip()}\")\n",
    "\n",
    "if files_with_issues:\n",
    "    raise RuntimeError(\"Legacy print syntax detected:\n",
    "\" + \"\n",
    "\".join(files_with_issues))\n",
    "else:\n",
    "    print('✅ No legacy print statements detected.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940c71a9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id='appendix'></a>Appendix\n",
    "\n",
    "- **Source Notebooks**: `blackjack_ai_colab.ipynb` and `blackjack_ai_train_eval.ipynb` were merged into this comprehensive hyper-notebook.\n",
    "- **Diagnostics**: Ensure the diagnostics section is executed after any custom modifications that might introduce legacy print statements."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
